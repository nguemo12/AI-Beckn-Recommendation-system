{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguemo12/AI-Beckn-Recommendation-system/blob/master/pw4_eval_ie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c124e2-aa30-4e39-bf27-e70462f12c3a",
      "metadata": {
        "id": "e5c124e2-aa30-4e39-bf27-e70462f12c3a"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PaulLerner/aivancity_nlp/blob/main/pw4_eval_ie.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2dec2e-d98d-4204-8e58-9401bcc6f4fe",
      "metadata": {
        "id": "7d2dec2e-d98d-4204-8e58-9401bcc6f4fe"
      },
      "source": [
        "# Installation and imports\n",
        "\n",
        "Hit `Ctrl+S` to save a copy of the Colab notebook to your drive\n",
        "\n",
        "Run on Google Colab GPU:\n",
        "- Connect\n",
        "- Modify execution\n",
        "- GPU\n",
        "\n",
        "![image.png](https://paullerner.github.io/aivancity_nlp/_static/colab_gpu.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54c3151-daea-4366-9b0c-080122b1f2bb",
      "metadata": {
        "id": "e54c3151-daea-4366-9b0c-080122b1f2bb",
        "outputId": "9f2b8ff6-6a4e-410e-cc31-713ae6afb2b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paul/miniforge3/envs/matos/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f972d2-064d-4350-a9b3-ad6a6301214e",
      "metadata": {
        "id": "80f972d2-064d-4350-a9b3-ad6a6301214e"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available(), \"Connect to GPU and try again (ask teacher for help)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b589742-a07b-49a2-a48a-9fb8a9745427",
      "metadata": {
        "id": "9b589742-a07b-49a2-a48a-9fb8a9745427"
      },
      "source": [
        "# Data\n",
        "\n",
        "We'll use the [E3C dataset](https://books.openedition.org/aaccademia/pdf/8663) of Named Entity Recognition in clinical texts.\n",
        "\n",
        "Therefore, we'll use a model pretrained on scientific texts: [SciBert](https://www.aclweb.org/anthology/D19-1371)\n",
        "\n",
        "## Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ad6037-1a47-44f4-8ce1-0561a239b503",
      "metadata": {
        "id": "b9ad6037-1a47-44f4-8ce1-0561a239b503",
        "outputId": "023e4ba9-c230-4bf1-ef22-544f183ab642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-21 16:21:46--  https://raw.githubusercontent.com/hltfbk/E3C-Corpus/refs/heads/main/preprocessed_data/clinical_entities/layer1/English/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 OK\n",
            "Length: 136768 (134K) [text/plain]\n",
            "Saving to: 'test.txt'\n",
            "\n",
            "test.txt            100%[===================>] 133.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-11-21 16:21:47 (4.57 MB/s) - 'test.txt' saved [136768/136768]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/hltfbk/E3C-Corpus/refs/heads/main/preprocessed_data/clinical_entities/layer1/English/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ae2234-5654-4389-9f40-afa81b6a48a4",
      "metadata": {
        "id": "b9ae2234-5654-4389-9f40-afa81b6a48a4",
        "outputId": "136d978b-5c94-4fb0-8464-8e92299478a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-21 16:21:47--  https://raw.githubusercontent.com/hltfbk/E3C-Corpus/refs/heads/main/preprocessed_data/clinical_entities/layer1/English/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 OK\n",
            "Length: 103512 (101K) [text/plain]\n",
            "Saving to: 'train.txt'\n",
            "\n",
            "train.txt           100%[===================>] 101.09K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-11-21 16:21:47 (6.99 MB/s) - 'train.txt' saved [103512/103512]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/hltfbk/E3C-Corpus/refs/heads/main/preprocessed_data/clinical_entities/layer1/English/train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9e0fb8-1bbd-421e-b11e-69cb486f1b81",
      "metadata": {
        "id": "df9e0fb8-1bbd-421e-b11e-69cb486f1b81"
      },
      "outputs": [],
      "source": [
        "encoder = AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\", device_map=\"auto\", add_pooling_layer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c424f5-6fe5-4ffe-90dc-2b6c62554af6",
      "metadata": {
        "id": "12c424f5-6fe5-4ffe-90dc-2b6c62554af6",
        "outputId": "b0d3f201-afd1-415b-d210-356730b6d8fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(31116, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b55d4f-406e-46bf-ba91-b05b1a86e968",
      "metadata": {
        "id": "a7b55d4f-406e-46bf-ba91-b05b1a86e968"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\", do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9da0a9-9975-495d-af73-19eac45f25e8",
      "metadata": {
        "id": "8a9da0a9-9975-495d-af73-19eac45f25e8"
      },
      "outputs": [],
      "source": [
        "LABELS = {'O': 0, 'B-ety': 1, 'I-ety': 2}\n",
        "i2label = {i: label for label, i in LABELS.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91a6918-4d31-47a9-a22b-157dc88d93f9",
      "metadata": {
        "id": "f91a6918-4d31-47a9-a22b-157dc88d93f9"
      },
      "outputs": [],
      "source": [
        "def read_data(path):\n",
        "    with open(path,\"rt\") as file:\n",
        "        lines = file.read().strip().split(\"\\n\")\n",
        "    dataset = []\n",
        "    tokens, labels = [], []\n",
        "    for line in lines:\n",
        "        if line:\n",
        "            token, label = line.split(\" \")\n",
        "            tokens.append(token)\n",
        "            labels.append(LABELS[label])\n",
        "        else:\n",
        "            dataset.append((tokens, labels))\n",
        "            tokens, labels = [], []\n",
        "    dataset.append((tokens, labels))\n",
        "    return dataset\n",
        "train_set = read_data(\"train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c456e186-6219-4e54-852b-8e3988a356c9",
      "metadata": {
        "id": "c456e186-6219-4e54-852b-8e3988a356c9"
      },
      "outputs": [],
      "source": [
        "test_set = read_data(\"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadc5f9c-bf58-4861-bfa8-fbb2716d5807",
      "metadata": {
        "id": "fadc5f9c-bf58-4861-bfa8-fbb2716d5807",
        "outputId": "8188057d-a2ba-43b3-b45d-c4a642ac67e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "669 851\n"
          ]
        }
      ],
      "source": [
        "print(len(train_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45e8da6-0438-413a-8b88-8e545af6c313",
      "metadata": {
        "id": "f45e8da6-0438-413a-8b88-8e545af6c313",
        "outputId": "b93312a5-71e8-484d-ce53-7fbf60706759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A 14 - year old boy with no significant past medical history presented to a small district hospital in southern Sierra Leone with a 4 day history of facial puffiness , peripheral pitting oedema , abdominal pains , and reduced urine output .\n",
            "[('A', 'O'), ('14', 'O'), ('-', 'O'), ('year', 'O'), ('old', 'O'), ('boy', 'O'), ('with', 'O'), ('no', 'O'), ('significant', 'O'), ('past', 'O'), ('medical', 'O'), ('history', 'O'), ('presented', 'O'), ('to', 'O'), ('a', 'O'), ('small', 'O'), ('district', 'O'), ('hospital', 'O'), ('in', 'O'), ('southern', 'O'), ('Sierra', 'O'), ('Leone', 'O'), ('with', 'O'), ('a', 'O'), ('4', 'O'), ('day', 'O'), ('history', 'O'), ('of', 'O'), ('facial', 'B-ety'), ('puffiness', 'I-ety'), (',', 'O'), ('peripheral', 'O'), ('pitting', 'B-ety'), ('oedema', 'I-ety'), (',', 'O'), ('abdominal', 'O'), ('pains', 'O'), (',', 'O'), ('and', 'O'), ('reduced', 'O'), ('urine', 'O'), ('output', 'O'), ('.', 'O')]\n"
          ]
        }
      ],
      "source": [
        "item = train_set[0]\n",
        "print(\" \".join(item[0]))\n",
        "print(list(zip(item[0], [i2label[label] for label in item[1]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c038d2-c67d-49e4-a7e6-46aee2680d2e",
      "metadata": {
        "id": "63c038d2-c67d-49e4-a7e6-46aee2680d2e"
      },
      "source": [
        "## Train-dev-test split\n",
        "Notice that the original dataset has no dedicated validation split.\n",
        "\n",
        "Randomly split the test set in 50% validation and 50% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028fd40e-c3e4-47d7-a49e-909e93ae0dca",
      "metadata": {
        "id": "028fd40e-c3e4-47d7-a49e-909e93ae0dca",
        "outputId": "fe389d38-e74b-4a8b-a6d3-c9bfa881198c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "669 425 426\n"
          ]
        }
      ],
      "source": [
        "print(len(train_set), len(dev_set), len(test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d17a67e-55ac-4b25-b8d8-fb2c554dd96b",
      "metadata": {
        "id": "3d17a67e-55ac-4b25-b8d8-fb2c554dd96b"
      },
      "source": [
        "## Sequence labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9f4548-a15a-486e-8943-753c5f172bc9",
      "metadata": {
        "id": "df9f4548-a15a-486e-8943-753c5f172bc9"
      },
      "source": [
        "Because BERT has a subword tokenizer, we need to map the label sequence to the subwords.\n",
        "\n",
        "Beware to change the label from B to I for subwords that are in the middle of a word.\n",
        "\n",
        "For example `'pit', '##ting'` should be tagged `'B-ety', 'I-ety'`, not `'B-ety', 'B-ety'`\n",
        "\n",
        "Also make sure to add BERT special tokens for beginning of sequence and end of sequence. Give them `-100` as label so they are not taken into account when computing the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59d80e5-a47d-4579-a059-db96463998cb",
      "metadata": {
        "scrolled": true,
        "id": "a59d80e5-a47d-4579-a059-db96463998cb",
        "outputId": "973e2490-a340-4fa2-966b-909a180d05df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " '14',\n",
              " '-',\n",
              " 'year',\n",
              " 'old',\n",
              " 'boy',\n",
              " 'with',\n",
              " 'no',\n",
              " 'significant',\n",
              " 'past',\n",
              " 'medical',\n",
              " 'history',\n",
              " 'presented',\n",
              " 'to',\n",
              " 'a',\n",
              " 'small',\n",
              " 'district',\n",
              " 'hospital',\n",
              " 'in',\n",
              " 'southern',\n",
              " 'Sie',\n",
              " '##rr',\n",
              " '##a',\n",
              " 'Leon',\n",
              " '##e',\n",
              " 'with',\n",
              " 'a',\n",
              " '4',\n",
              " 'day',\n",
              " 'history',\n",
              " 'of',\n",
              " 'facial',\n",
              " 'pu',\n",
              " '##ffi',\n",
              " '##ness',\n",
              " ',',\n",
              " 'peripheral',\n",
              " 'pit',\n",
              " '##ting',\n",
              " 'o',\n",
              " '##edema',\n",
              " ',',\n",
              " 'abdominal',\n",
              " 'pain',\n",
              " '##s',\n",
              " ',',\n",
              " 'and',\n",
              " 'reduced',\n",
              " 'urine',\n",
              " 'output',\n",
              " '.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(\" \".join(item[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e97cc7-2948-4556-adfe-3d1b62518081",
      "metadata": {
        "id": "59e97cc7-2948-4556-adfe-3d1b62518081",
        "outputId": "3153cdc7-273b-4939-e108-2301b5e397d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS]'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.cls_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb8f9c2-0877-41f3-a060-89d25131f665",
      "metadata": {
        "id": "3cb8f9c2-0877-41f3-a060-89d25131f665",
        "outputId": "091c0155-9943-4244-e5cd-4793b6a194fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[SEP]'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.sep_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55b57c4-69a4-409c-a5ce-d9786e3c550f",
      "metadata": {
        "id": "d55b57c4-69a4-409c-a5ce-d9786e3c550f",
        "outputId": "0d9e58e0-3185-454b-8f17-3c98b7cc2d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('[CLS]', None), ('A', 'O'), ('14', 'O'), ('-', 'O'), ('year', 'O'), ('old', 'O'), ('boy', 'O'), ('with', 'O'), ('no', 'O'), ('significant', 'O'), ('past', 'O'), ('medical', 'O'), ('history', 'O'), ('presented', 'O'), ('to', 'O'), ('a', 'O'), ('small', 'O'), ('district', 'O'), ('hospital', 'O'), ('in', 'O'), ('southern', 'O'), ('Sie', 'O'), ('##rr', 'O'), ('##a', 'O'), ('Leon', 'O'), ('##e', 'O'), ('with', 'O'), ('a', 'O'), ('4', 'O'), ('day', 'O'), ('history', 'O'), ('of', 'O'), ('facial', 'B-ety'), ('pu', 'I-ety'), ('##ffi', 'I-ety'), ('##ness', 'I-ety'), (',', 'O'), ('peripheral', 'O'), ('pit', 'B-ety'), ('##ting', 'I-ety'), ('o', 'I-ety'), ('##edema', 'I-ety'), (',', 'O'), ('abdominal', 'O'), ('pain', 'O'), ('##s', 'O'), (',', 'O'), ('and', 'O'), ('reduced', 'O'), ('urine', 'O'), ('output', 'O'), ('.', 'O'), ('[SEP]', None)]\n"
          ]
        }
      ],
      "source": [
        "# expected output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c924cc6-c7c1-40cb-8a9c-96b6fed599ee",
      "metadata": {
        "id": "0c924cc6-c7c1-40cb-8a9c-96b6fed599ee"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220cd191-7ca8-4762-9c53-75f229bd0e43",
      "metadata": {
        "id": "220cd191-7ca8-4762-9c53-75f229bd0e43"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73a5112-2cd8-447b-8abe-a2b9ad732ab6",
      "metadata": {
        "id": "e73a5112-2cd8-447b-8abe-a2b9ad732ab6"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a60f2df-82d7-4a69-ab48-08cdf0e493a2",
      "metadata": {
        "id": "2a60f2df-82d7-4a69-ab48-08cdf0e493a2"
      },
      "source": [
        "Tokenize and get labels for all examples in the batch. Get the identifiers of tokens using `tokenizer.convert_tokens_to_ids`\n",
        "\n",
        "Save the length (number of tokens) of each example in a separate list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25fc000-a095-4f8f-9986-38ec0ee28222",
      "metadata": {
        "id": "c25fc000-a095-4f8f-9986-38ec0ee28222"
      },
      "source": [
        "You should end up with a list of list like so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b00450e-8269-487b-b75f-a30c722bda93",
      "metadata": {
        "scrolled": true,
        "id": "5b00450e-8269-487b-b75f-a30c722bda93",
        "outputId": "7f5aae4c-95e2-4e24-81cc-fb79becc34d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[101, 22690, 106, 163, 3055, 28224, 171, 211, 102],\n",
              " [101,\n",
              "  5879,\n",
              "  578,\n",
              "  813,\n",
              "  136,\n",
              "  3174,\n",
              "  864,\n",
              "  689,\n",
              "  806,\n",
              "  603,\n",
              "  6786,\n",
              "  23642,\n",
              "  855,\n",
              "  111,\n",
              "  803,\n",
              "  939,\n",
              "  2015,\n",
              "  211,\n",
              "  102],\n",
              " [101,\n",
              "  186,\n",
              "  22950,\n",
              "  253,\n",
              "  105,\n",
              "  2579,\n",
              "  953,\n",
              "  2477,\n",
              "  125,\n",
              "  198,\n",
              "  28351,\n",
              "  30106,\n",
              "  1003,\n",
              "  6118,\n",
              "  8970,\n",
              "  2212,\n",
              "  7108,\n",
              "  15307,\n",
              "  7290,\n",
              "  211,\n",
              "  102],\n",
              " [101,\n",
              "  186,\n",
              "  1476,\n",
              "  11732,\n",
              "  1337,\n",
              "  125,\n",
              "  105,\n",
              "  5632,\n",
              "  18459,\n",
              "  430,\n",
              "  596,\n",
              "  907,\n",
              "  537,\n",
              "  11708,\n",
              "  430,\n",
              "  15184,\n",
              "  968,\n",
              "  430,\n",
              "  251,\n",
              "  9551,\n",
              "  11012,\n",
              "  7026,\n",
              "  211,\n",
              "  102]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92382a16-4ae5-4099-addb-ae930e9e2350",
      "metadata": {
        "scrolled": true,
        "id": "92382a16-4ae5-4099-addb-ae930e9e2350",
        "outputId": "3b19238d-139a-48f6-9354-83b60b73f8a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[-100, 0, 0, 0, 0, 0, 0, 0, -100],\n",
              " [-100, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, -100],\n",
              " [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
              " [-100,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  2,\n",
              "  2,\n",
              "  0,\n",
              "  -100]]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b30857-c8d6-4460-a408-c34e73a00924",
      "metadata": {
        "id": "d4b30857-c8d6-4460-a408-c34e73a00924"
      },
      "source": [
        "Notice that each example in the batch has a different size so we will pad them. The padded tokens will be masked in the attention mechanism (already implemented in BERT, you simply need to pass `attention_mask`) and will be given the label `-100` so they are not taken into account when computing the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63839c35-175b-43a0-ac12-419186923fcf",
      "metadata": {
        "id": "63839c35-175b-43a0-ac12-419186923fcf",
        "outputId": "99e61f54-be9c-40c7-f9f9-a2e993a35237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[9, 19, 21, 24]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a36ea0d-70af-401e-9549-9d7f42a95b2f",
      "metadata": {
        "id": "1a36ea0d-70af-401e-9549-9d7f42a95b2f"
      },
      "source": [
        "Here we simply add padding so that all examples can fit in the same Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f31a18-9219-43fa-bb8d-546fdc6e6e6d",
      "metadata": {
        "id": "a4f31a18-9219-43fa-bb8d-546fdc6e6e6d"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.zeros(len(batch), max(lengths), dtype=int, device=\"cuda\")\n",
        "attention_mask = torch.zeros(len(batch), max(lengths), dtype=int, device=\"cuda\")\n",
        "labels = torch.full((len(batch), max(lengths)), -100, dtype=int, device=\"cuda\")\n",
        "\n",
        "for i, (token, label) in enumerate(zip(tokens_batch, labels_batch)):\n",
        "    input_ids[i, :len(token)] = torch.tensor(token)\n",
        "    attention_mask[i, :len(token)] = 1\n",
        "    labels[i, :len(label)] = torch.tensor(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5596f804-7ae4-449c-aca0-c39b8c7880c7",
      "metadata": {
        "id": "5596f804-7ae4-449c-aca0-c39b8c7880c7"
      },
      "source": [
        "Voilà!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455b4552-cd3f-4b0c-bbd7-029a094c176c",
      "metadata": {
        "id": "455b4552-cd3f-4b0c-bbd7-029a094c176c",
        "outputId": "f7f2c70e-01ce-4a16-d86a-8a3fc8736521"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 24]), torch.Size([4, 24]), torch.Size([4, 24]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, attention_mask.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0b37c0-2565-4ee1-beb8-304e2d4a44b7",
      "metadata": {
        "id": "ae0b37c0-2565-4ee1-beb8-304e2d4a44b7",
        "outputId": "92966275-d40e-4862-96f8-8d2db0771c6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101, 22690,   106,   163,  3055, 28224,   171,   211,   102,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0],\n",
              "        [  101,  5879,   578,   813,   136,  3174,   864,   689,   806,   603,\n",
              "          6786, 23642,   855,   111,   803,   939,  2015,   211,   102,     0,\n",
              "             0,     0,     0,     0],\n",
              "        [  101,   186, 22950,   253,   105,  2579,   953,  2477,   125,   198,\n",
              "         28351, 30106,  1003,  6118,  8970,  2212,  7108, 15307,  7290,   211,\n",
              "           102,     0,     0,     0],\n",
              "        [  101,   186,  1476, 11732,  1337,   125,   105,  5632, 18459,   430,\n",
              "           596,   907,   537, 11708,   430, 15184,   968,   430,   251,  9551,\n",
              "         11012,  7026,   211,   102]], device='cuda:0')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59e96d7-2fd8-42a9-a887-ba9f960e9297",
      "metadata": {
        "id": "e59e96d7-2fd8-42a9-a887-ba9f960e9297",
        "outputId": "3002f0e5-2835-4f88-be50-214bfde375a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-100,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              "        [-100,    0,    0,    0,    0,    0,    0,    1,    2,    2,    2,    0,\n",
              "            0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100],\n",
              "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100],\n",
              "        [-100,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
              "            0,    1,    0,    1,    2,    0,    0,    1,    2,    2,    0, -100]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706e519a-ff5a-4fef-8310-68ba3350ee6a",
      "metadata": {
        "id": "706e519a-ff5a-4fef-8310-68ba3350ee6a",
        "outputId": "b6ee382d-d00a-416f-c020-07278f5d4776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c53ff5d-b397-4054-be13-7481110115ca",
      "metadata": {
        "id": "2c53ff5d-b397-4054-be13-7481110115ca"
      },
      "source": [
        "# Model\n",
        "## Encoding with BERT\n",
        "\n",
        "BERT provides contextualized embeddings for all tokens in the batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3531193-7362-4125-9cc0-3dc457733408",
      "metadata": {
        "id": "f3531193-7362-4125-9cc0-3dc457733408"
      },
      "outputs": [],
      "source": [
        "output = encoder(input_ids=input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbca7b4-a90a-4d65-a82b-8319400d03d9",
      "metadata": {
        "id": "3bbca7b4-a90a-4d65-a82b-8319400d03d9",
        "outputId": "158270c9-4fab-4274-8ca7-2afa0f54d30e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 39, 768])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (batch size, sequence length, embedding dimension)\n",
        "output.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed29cfc3-148a-405d-b453-dac541aa8b16",
      "metadata": {
        "id": "ed29cfc3-148a-405d-b453-dac541aa8b16"
      },
      "source": [
        "## Sequence Tagging\n",
        "For Named Entity Recognition, we use a simple tagging model: we simply add a linear layer on top of the encoder for multi-class classification:\n",
        "- What is the input dimension of the classifier?\n",
        "- How many classes are there?\n",
        "\n",
        "Note: this Linear classifier is really simple and **assigns a label to each token independently**. A better way to do Named Entity Recognition that we did not cover in class is to use Conditional Random Field (CRF) for **structured prediction** (basically, assign the label of a token based on the other labels already assigned).\n",
        "\n",
        "Note 2: HuggingFace's `transformers` provides built-in sequence tagging models like https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForTokenClassification which you are not allowed to use until the end of this class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a8d13d-bc40-4770-9746-2169086dbeb1",
      "metadata": {
        "id": "03a8d13d-bc40-4770-9746-2169086dbeb1"
      },
      "outputs": [],
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        raise NotImplementedError()\n",
        "        self.classifier = TODO\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embeddings = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4425ed87-f481-410f-b70f-48af496e1d33",
      "metadata": {
        "id": "4425ed87-f481-410f-b70f-48af496e1d33"
      },
      "outputs": [],
      "source": [
        "tagger = Tagger(encoder).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2b3d00-3bf0-45a1-90ab-2cfc08b4dc57",
      "metadata": {
        "id": "bc2b3d00-3bf0-45a1-90ab-2cfc08b4dc57"
      },
      "outputs": [],
      "source": [
        "logits = tagger(input_ids, attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f88bfc-9696-4424-95e8-e130ca152b91",
      "metadata": {
        "id": "14f88bfc-9696-4424-95e8-e130ca152b91",
        "outputId": "e11ae3ea-4472-4e40-dbbc-3d1e81c51044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 24, 3])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (batch size, sequence length, number of classes)\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bae39df-5087-4ca2-bc75-5b9c0b3d94f5",
      "metadata": {
        "id": "8bae39df-5087-4ca2-bc75-5b9c0b3d94f5"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Use Cross-entropy to train the classifier. First compute the loss on a single batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecb085a-62e6-4089-8956-1fdf1088bf4b",
      "metadata": {
        "id": "4ecb085a-62e6-4089-8956-1fdf1088bf4b",
        "outputId": "87e88803-5414-48eb-b9c4-5574b20ab782"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0238, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# value may vary but you might get a loss like this one"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01791841-3c1d-4bda-ad3b-3c96884d89f4",
      "metadata": {
        "id": "01791841-3c1d-4bda-ad3b-3c96884d89f4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c336e5ba-6c8f-466a-bf55-409ebc6d4eec",
      "metadata": {
        "id": "c336e5ba-6c8f-466a-bf55-409ebc6d4eec"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3201f57-9850-40d5-a370-b3648725e5a1",
      "metadata": {
        "id": "a3201f57-9850-40d5-a370-b3648725e5a1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(\"logs/pw4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4879c4f-2931-45ec-9ab7-56f65065ea8e",
      "metadata": {
        "id": "b4879c4f-2931-45ec-9ab7-56f65065ea8e"
      },
      "source": [
        "Run tensorboard before training. Refresh during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd9a77a-c425-400a-a3e9-f04b9b825f39",
      "metadata": {
        "id": "9cd9a77a-c425-400a-a3e9-f04b9b825f39",
        "outputId": "4800f688-c8a9-4324-9dcf-98432b44b4bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-85689103f81553f1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-85689103f81553f1\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs/pw4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f422710-3993-414a-9f3b-85a834504e5c",
      "metadata": {
        "id": "2f422710-3993-414a-9f3b-85a834504e5c"
      },
      "outputs": [],
      "source": [
        "tagger = Tagger(encoder).cuda()\n",
        "# gradient checkpoint: lower memory footprint but you need to compute forward passes twice\n",
        "tagger.encoder.gradient_checkpointing_enable()\n",
        "\n",
        "loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(tagger.parameters(), lr=0.0001)\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
        "validation_loader = torch.utils.data.DataLoader(dev_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
        "\n",
        "steps = 0\n",
        "for epoch in range(5):\n",
        "    for batch in train_loader:\n",
        "        raise NotImplementedError(\"Format batch and compute loss as you did above\")\n",
        "        loss = TODO\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", loss.item(), steps)\n",
        "        steps += 1\n",
        "        loss.backward()\n",
        "        # gradient clipping to avoid exploding gradients\n",
        "        nn.utils.clip_grad_norm_(tagger.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    # validation\n",
        "    with torch.no_grad():\n",
        "        tagger.eval()\n",
        "        valid_loss = 0\n",
        "        valid_batches = 0\n",
        "        for batch in validation_loader:\n",
        "            raise NotImplementedError(\"Format batch and compute loss as you did above\")\n",
        "            loss = TODO\n",
        "            valid_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "        tagger.train()\n",
        "        writer.add_scalar(\"Loss/validation\", valid_loss/valid_batches, epoch)\n",
        "\n",
        "    # saving checkpoint\n",
        "    torch.save(tagger.state_dict(), f\"tagger_{epoch}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12b7c7b-af7d-4bb8-a750-e364bcc8d8f1",
      "metadata": {
        "id": "f12b7c7b-af7d-4bb8-a750-e364bcc8d8f1"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Notice that the model quickly overfits (after 1 epoch?) given the small size of the training steps. Load the best checkpoint according to the validation loss (epoch 1 in my case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1c7e26-3333-418f-930a-ac7b76dff4ee",
      "metadata": {
        "id": "db1c7e26-3333-418f-930a-ac7b76dff4ee",
        "outputId": "c5624faf-a5f1-40be-d1d1-c2ed52a9c38f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9055/4253622363.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tagger.load_state_dict(torch.load(\"tagger_0.pt\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Tagger(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagger.load_state_dict(torch.load(\"tagger_0.pt\"))\n",
        "tagger.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470bf08a-c0ed-45f7-a17f-f6ec45dc5712",
      "metadata": {
        "id": "470bf08a-c0ed-45f7-a17f-f6ec45dc5712",
        "outputId": "e3de7c58-0ecc-4254-a84b-523b16f7bb8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'O': 0, 'B-ety': 1, 'I-ety': 2}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LABELS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc589eb3-c34d-4e24-8bf7-da00cc594d64",
      "metadata": {
        "id": "cc589eb3-c34d-4e24-8bf7-da00cc594d64"
      },
      "source": [
        "Now run your model on your test set. This time we're not only interested in a low loss but in the actual predictions of the models:\n",
        "- How to get the class predicted by your model on each token?\n",
        "\n",
        "Once you get classes, compute:\n",
        "- precision (of classifying 'B-ety' and 'I-ety')\n",
        "- recall (of classifying 'B-ety' and 'I-ety')\n",
        "- F1-score (of classifying 'B-ety' and 'I-ety')\n",
        "\n",
        "Bonus: what is the issue with computing scores on tokens tokenized using BERT tokenizer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d41849-2fb2-420c-b474-3981122d29e2",
      "metadata": {
        "scrolled": true,
        "id": "e1d41849-2fb2-420c-b474-3981122d29e2",
        "outputId": "888bbf35-3cf4-4586-a09a-40d4f735112e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('[CLS]', 'O'),\n",
              " ('Given', 'O'),\n",
              " ('the', 'O'),\n",
              " ('positive', 'O'),\n",
              " ('screen', 'O'),\n",
              " ('for', 'O'),\n",
              " ('ce', 'B-ety'),\n",
              " ('##li', 'I-ety'),\n",
              " ('##ac', 'I-ety'),\n",
              " ('disease', 'I-ety'),\n",
              " ('(', 'O'),\n",
              " ('positive', 'O'),\n",
              " ('anti', 'O'),\n",
              " ('-', 'O'),\n",
              " ('tissue', 'O'),\n",
              " ('trans', 'O'),\n",
              " ('##glut', 'O'),\n",
              " ('##aminase', 'O'),\n",
              " ('antibodies', 'O'),\n",
              " ('and', 'O'),\n",
              " ('results', 'O'),\n",
              " ('of', 'O'),\n",
              " ('duoden', 'O'),\n",
              " ('##al', 'O'),\n",
              " ('biopsy', 'O'),\n",
              " (')', 'O'),\n",
              " (',', 'O'),\n",
              " ('dietary', 'O'),\n",
              " ('intervention', 'O'),\n",
              " ('was', 'O'),\n",
              " ('immediately', 'O'),\n",
              " ('comm', 'O'),\n",
              " ('##enced', 'O'),\n",
              " ('.', 'O'),\n",
              " ('[SEP]', 'O')]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example prediction of my model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d383ab-496c-44a1-8939-721eff7476b4",
      "metadata": {
        "id": "17d383ab-496c-44a1-8939-721eff7476b4"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
        "for batch in test_loader:\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdc836f-2228-42be-9c6d-282d29dd896b",
      "metadata": {
        "id": "0bdc836f-2228-42be-9c6d-282d29dd896b"
      },
      "source": [
        "The result is pretty bad but consider we use a very small training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325be978-d796-4c5b-b7af-e71f90e0b314",
      "metadata": {
        "id": "325be978-d796-4c5b-b7af-e71f90e0b314",
        "outputId": "84531116-e994-4142-869f-f0822e3dd869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision=61.3%, recall=29.3%, f_score=39.7%\n"
          ]
        }
      ],
      "source": [
        "print(f\"{precision=:.1%}, {recall=:.1%}, {f_score=:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50008d4-fada-412c-84ad-19077e469a13",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "e50008d4-fada-412c-84ad-19077e469a13"
      },
      "source": [
        "# Bonus\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "- Resplit the data by keeping the 426 longest texts as out-of-distribution test set. The remaining can be randomly split in train-validation.\n",
        "- Compute F1 score using detokenized outputs\n",
        "\n",
        "## Parameter-efficient fine-tuning\n",
        "\n",
        "Compare full fine-tuning like above with LoRA\n",
        "\n",
        "## More encoders\n",
        "\n",
        "Evaluate other models, e.g.:\n",
        "- another encoder, e.g. BERT or RoBERTa that were trained on general domain texts or https://huggingface.co/medicalai/ClinicalBERT that was trained on clinical texts\n",
        "- a decoder-only like GPT-2\n",
        "- an encoder-decoder like BART\n",
        "\n",
        "## Better classifiers\n",
        "\n",
        "use Conditional Random Field (CRF) instead of a linear classifier"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}